{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74b81270-c156-4967-9b6f-7d6bce8fa371",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2041919318.py, line 289)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 289\u001b[1;36m\u001b[0m\n\u001b[1;33m    .appName(\"SparkRDDParquet\") \\\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Script de comparación de performance entre distintas librerías/sistemas:\n",
    "\n",
    "1. PySpark Dataframes (CSV y Parquet)\n",
    "2. PySpark RDD (CSV y Parquet)\n",
    "3. Pandas (CSV y Parquet)\n",
    "4. Polars (CSV y Parquet)\n",
    "\n",
    "Autoría original:\n",
    "- Pablo Nieto Rodríguez\n",
    "- Pablo Fontádez\n",
    "- Christian Berdejo Sánchez\n",
    "\n",
    "Adaptado para un único script y organizado en funciones.\n",
    "\n",
    "Se miden los tiempos de:\n",
    "1. Lectura\n",
    "2. Filtrado (Arrest = true)\n",
    "3. Agrupación (Group by District)\n",
    "4. Conteo\n",
    "5. Ordenamiento\n",
    "\n",
    "Al final se muestra un gráfico comparativo con los tiempos medidos.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "\n",
    "############################\n",
    "#         FUNCIONES        #\n",
    "############################\n",
    "\n",
    "def measure_spark_df_csv(path_file):\n",
    "    \"\"\"\n",
    "    Mide los tiempos de lectura, filtrado, group by, count y sort usando Spark DataFrame con un archivo CSV.\n",
    "    Devuelve un diccionario con cada operación y su tiempo.\n",
    "    \"\"\"\n",
    "    spark_session = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"SparkDataFrameCSV\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark_session.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    tiempos = {}\n",
    "\n",
    "    # Lectura\n",
    "    start_time = time.time()\n",
    "    data_frame = (spark_session.read\n",
    "                               .options(header='true', inferschema='true')\n",
    "                               .option(\"delimiter\", \",\")\n",
    "                               .option(\"timestampFormat\", \"yyyy-MM-dd\")\n",
    "                               .csv(path_file)\n",
    "                               .persist())\n",
    "    tiempos[\"Reading\"] = time.time() - start_time\n",
    "\n",
    "    # Filtrado\n",
    "    start_time = time.time()\n",
    "    filtered_df = data_frame.filter(\"Arrest = true\")\n",
    "    tiempos[\"Filtering\"] = time.time() - start_time\n",
    "\n",
    "    # Group by\n",
    "    start_time = time.time()\n",
    "    grouped_df = filtered_df.groupby(\"District\")\n",
    "    tiempos[\"Grouping\"] = time.time() - start_time\n",
    "\n",
    "    # Count\n",
    "    start_time = time.time()\n",
    "    counted_df = grouped_df.count()\n",
    "    tiempos[\"Counting\"] = time.time() - start_time\n",
    "\n",
    "    # Sort\n",
    "    start_time = time.time()\n",
    "    sorted_df = counted_df.orderBy(\"count\", ascending=False)\n",
    "    tiempos[\"Sorting\"] = time.time() - start_time\n",
    "\n",
    "    return tiempos\n",
    "\n",
    "\n",
    "def measure_spark_rdd_csv(path_file):\n",
    "    \"\"\"\n",
    "    Mide los tiempos de lectura, filtrado, group by, count y sort usando Spark RDD con un archivo CSV.\n",
    "    Devuelve un diccionario con cada operación y su tiempo.\n",
    "    \"\"\"\n",
    "    spark_session = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"SparkRDDCSV\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark_context = spark_session.sparkContext\n",
    "    spark_context.setLogLevel(\"ERROR\")\n",
    "\n",
    "    tiempos = {}\n",
    "\n",
    "    # Lectura\n",
    "    start_time = time.time()\n",
    "    rdd = spark_context.textFile(path_file)\n",
    "    tiempos[\"Reading\"] = time.time() - start_time\n",
    "\n",
    "    header = rdd.first()\n",
    "    columns = header.split(\",\")\n",
    "\n",
    "    try:\n",
    "        arrest_idx = columns.index(\"Arrest\")\n",
    "        district_idx = columns.index(\"District\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Verifica que las columnas 'Arrest' y 'District' existan en el CSV.\")\n",
    "\n",
    "    rdd_no_header = rdd.filter(lambda line: line != header)\n",
    "    rdd_parsed = rdd_no_header.map(lambda line: line.split(\",\"))\n",
    "\n",
    "    # Filtrado\n",
    "    start_time = time.time()\n",
    "    rdd_filtered = rdd_parsed.filter(\n",
    "        lambda row: len(row) > max(arrest_idx, district_idx) and row[arrest_idx].lower() == \"true\"\n",
    "    )\n",
    "    tiempos[\"Filtering\"] = time.time() - start_time\n",
    "\n",
    "    # Group by (aquí en realidad se mapea a (district,1))\n",
    "    start_time = time.time()\n",
    "    rdd_mapped = rdd_filtered.map(lambda row: (row[district_idx], 1))\n",
    "    tiempos[\"Grouping\"] = time.time() - start_time\n",
    "\n",
    "    # Count (reduceByKey)\n",
    "    start_time = time.time()\n",
    "    rdd_counted = rdd_mapped.reduceByKey(lambda a, b: a + b)\n",
    "    tiempos[\"Counting\"] = time.time() - start_time\n",
    "\n",
    "    # Sort\n",
    "    start_time = time.time()\n",
    "    rdd_sorted = rdd_counted.sortBy(lambda x: x[1], ascending=False)\n",
    "    tiempos[\"Sorting\"] = time.time() - start_time\n",
    "\n",
    "    return tiempos\n",
    "\n",
    "\n",
    "def measure_pandas_csv(path_file):\n",
    "    \"\"\"\n",
    "    Mide los tiempos de lectura, filtrado, group by, count y sort usando Pandas con un archivo CSV.\n",
    "    Devuelve un diccionario con cada operación y su tiempo.\n",
    "    \"\"\"\n",
    "    tiempos = {}\n",
    "\n",
    "    # Lectura\n",
    "    start_time = time.time()\n",
    "    df = pd.read_csv(path_file, delimiter=\",\", low_memory=False)\n",
    "    tiempos[\"Reading\"] = time.time() - start_time\n",
    "\n",
    "    # Filtrado\n",
    "    start_time = time.time()\n",
    "    filtered_df = df[df[\"Arrest\"] == True]\n",
    "    tiempos[\"Filtering\"] = time.time() - start_time\n",
    "\n",
    "    # Agrupación\n",
    "    start_time = time.time()\n",
    "    grouped_df = filtered_df.groupby(\"District\").size()\n",
    "    tiempos[\"Grouping\"] = time.time() - start_time\n",
    "\n",
    "    # Conteo (en este caso, el size ya devolvió el conteo, pero separamos la medición)\n",
    "    start_time = time.time()\n",
    "    counted_df = grouped_df.reset_index(name=\"count\")\n",
    "    tiempos[\"Counting\"] = time.time() - start_time\n",
    "\n",
    "    # Ordenamiento\n",
    "    start_time = time.time()\n",
    "    sorted_df = counted_df.sort_values(by=\"count\", ascending=False)\n",
    "    tiempos[\"Sorting\"] = time.time() - start_time\n",
    "\n",
    "    return tiempos\n",
    "\n",
    "\n",
    "def measure_polars_csv(path_file):\n",
    "    \"\"\"\n",
    "    Mide los tiempos de lectura, filtrado, group by, count y sort usando Polars con un archivo CSV.\n",
    "    Devuelve un diccionario con cada operación y su tiempo.\n",
    "    \"\"\"\n",
    "    tiempos = {}\n",
    "\n",
    "    # Lectura\n",
    "    start_time = time.time()\n",
    "    df = pl.read_csv(path_file)\n",
    "    tiempos[\"Reading\"] = time.time() - start_time\n",
    "\n",
    "    # Filtrado\n",
    "    start_time = time.time()\n",
    "    filter_df = df.filter(pl.col(\"Arrest\") == True)\n",
    "    tiempos[\"Filtering\"] = time.time() - start_time\n",
    "\n",
    "    # Agrupación (group_by)\n",
    "    start_time = time.time()\n",
    "    grouped_df = filter_df.group_by(\"District\")\n",
    "    tiempos[\"Grouping\"] = time.time() - start_time\n",
    "\n",
    "    # Conteo\n",
    "    start_time = time.time()\n",
    "    count_df = grouped_df.agg(pl.count().alias(\"count\"))\n",
    "    tiempos[\"Counting\"] = time.time() - start_time\n",
    "\n",
    "    # Ordenamiento\n",
    "    start_time = time.time()\n",
    "    sort_df = count_df.sort(\"count\", descending=True)\n",
    "    tiempos[\"Sorting\"] = time.time() - start_time\n",
    "\n",
    "    return tiempos\n",
    "\n",
    "\n",
    "def transform_csv_to_parquet(path_file_csv, path_file_parquet=\"data_parquet\"):\n",
    "    \"\"\"\n",
    "    Lee un CSV usando Pandas y lo transforma a parquet (usando pyarrow) en la ruta especificada.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path_file_csv)\n",
    "    df.to_parquet(path_file_parquet, engine='pyarrow')\n",
    "\n",
    "\n",
    "def measure_spark_df_parquet(path_parquet):\n",
    "    \"\"\"\n",
    "    Mide los tiempos de lectura, filtrado, group by, count y sort usando Spark DataFrame con un archivo Parquet.\n",
    "    Devuelve un diccionario con cada operación y su tiempo.\n",
    "    \"\"\"\n",
    "    spark_session = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"SparkDataFrameParquet\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark_session.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    tiempos = {}\n",
    "\n",
    "    # Lectura\n",
    "    start_time = time.time()\n",
    "    data_frame = (spark_session.read\n",
    "                               .options(header='true', inferschema='true')\n",
    "                               .option(\"delimiter\", \",\")\n",
    "                               .option(\"timestampFormat\", \"yyyy-MM-dd\")\n",
    "                               .parquet(path_parquet)\n",
    "                               .persist())\n",
    "    tiempos[\"Reading\"] = time.time() - start_time\n",
    "\n",
    "    # Filtrado\n",
    "    start_time = time.time()\n",
    "    filtered_df = data_frame.filter(\"Arrest = true\")\n",
    "    tiempos[\"Filtering\"] = time.time() - start_time\n",
    "\n",
    "    # Group by\n",
    "    start_time = time.time()\n",
    "    grouped_df = filtered_df.groupby(\"District\")\n",
    "    tiempos[\"Grouping\"] = time.time() - start_time\n",
    "\n",
    "    # Count\n",
    "    start_time = time.time()\n",
    "    counted_df = grouped_df.count()\n",
    "    tiempos[\"Counting\"] = time.time() - start_time\n",
    "\n",
    "    # Sort\n",
    "    start_time = time.time()\n",
    "    sorted_df = counted_df.orderBy(\"count\", ascending=False)\n",
    "    tiempos[\"Sorting\"] = time.time() - start_time\n",
    "\n",
    "    return tiempos\n",
    "\n",
    "\n",
    "def measure_spark_rdd_parquet(path_parquet):\n",
    "    \"\"\"\n",
    "    Mide los tiempos de lectura, filtrado, group by, count y sort usando Spark RDD\n",
    "    leyendo primero un DataFrame parquet y convirtiéndolo a RDD.\n",
    "    Devuelve un diccionario con cada operación y su tiempo.\n",
    "    \"\"\"\n",
    "\n",
    "    conf = SparkConf() \\\n",
    "    .set(\"spark.driver.memory\", \"6g\") \\\n",
    "    .set(\"spark.executor.memory\", \"6g\") \\\n",
    "    .set(\"spark.network.timeout\", \"600s\") \\\n",
    "    .set(\"spark.executor.heartbeatInterval\", \"120s\")\n",
    "    \n",
    "    spark_session = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(conf=conf)\\\n",
    "        .appName(\"SparkRDDParquet\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark_context = spark_session.sparkContext\n",
    "    spark_context.setLogLevel(\"ERROR\")\n",
    "\n",
    "    tiempos = {}\n",
    "\n",
    "    # Lectura (Spark no lee parquet con RDD directamente, se lee DF y se convierte a RDD)\n",
    "    start_time = time.time()\n",
    "    spark_df = spark_session.read.parquet(path_parquet)\n",
    "    rdd = spark_df.rdd\n",
    "    tiempos[\"Reading\"] = time.time() - start_time\n",
    "\n",
    "    # Filtrado\n",
    "    start_time = time.time()\n",
    "    rdd_filtered = rdd.filter(lambda row: row.Arrest is not None and row.Arrest == True)\n",
    "    tiempos[\"Filtering\"] = time.time() - start_time\n",
    "\n",
    "    \n",
    "    # Group by (map -> reduceByKey)\n",
    "    start_time = time.time()\n",
    "    rdd_mapped = rdd_filtered.map(lambda row: (row.District, 1) \n",
    "    if row.District is not None else (\"Unknown\", 1))\n",
    "    tiempos[\"Grouping\"] = time.time() - start_time\n",
    "\n",
    "    # Count\n",
    "    start_time = time.time()\n",
    "    rdd_counted = rdd_mapped.reduceByKey(lambda a, b: a + b)\n",
    "    tiempos[\"Counting\"] = time.time() - start_time\n",
    "\n",
    "    # Sort\n",
    "    start_time = time.time()\n",
    "    rdd_sorted = rdd_counted.sortBy(lambda x: x[1], ascending=False)\n",
    "    tiempos[\"Sorting\"] = time.time() - start_time\n",
    "\n",
    "    return tiempos\n",
    "\n",
    "\n",
    "def measure_pandas_parquet(path_parquet):\n",
    "    \"\"\"\n",
    "    Mide los tiempos de lectura, filtrado, group by, count y sort usando Pandas con un archivo Parquet.\n",
    "    Devuelve un diccionario con cada operación y su tiempo.\n",
    "    \"\"\"\n",
    "    tiempos = {}\n",
    "\n",
    "    # Lectura\n",
    "    start_time = time.time()\n",
    "    df = pd.read_parquet(path_parquet, engine=\"pyarrow\")\n",
    "    tiempos[\"Reading\"] = time.time() - start_time\n",
    "\n",
    "    # Filtrado\n",
    "    start_time = time.time()\n",
    "    df_filtrado = df[df[\"Arrest\"] == True]\n",
    "    tiempos[\"Filtering\"] = time.time() - start_time\n",
    "\n",
    "    # Agrupación\n",
    "    start_time = time.time()\n",
    "    df_agrupado = df_filtrado.groupby(\"District\").size().reset_index(name=\"count\")\n",
    "    tiempos[\"Grouping\"] = time.time() - start_time\n",
    "\n",
    "    # Conteo (aquí está incluido en size() y reset_index)\n",
    "    start_time = time.time()\n",
    "    df_count = df_filtrado.groupby(\"District\")[\"Arrest\"].count()\n",
    "    tiempos[\"Counting\"] = time.time() - start_time\n",
    "\n",
    "    # Ordenamiento\n",
    "    start_time = time.time()\n",
    "    df_sorted = df_agrupado.sort_values(by=\"count\", ascending=False)\n",
    "    tiempos[\"Sorting\"] = time.time() - start_time\n",
    "\n",
    "    return tiempos\n",
    "\n",
    "\n",
    "def measure_polars_parquet(path_parquet):\n",
    "    \"\"\"\n",
    "    Mide los tiempos de lectura, filtrado, group by, count y sort usando Polars con un archivo Parquet.\n",
    "    Devuelve un diccionario con cada operación y su tiempo.\n",
    "    \"\"\"\n",
    "    tiempos = {}\n",
    "\n",
    "    # Lectura\n",
    "    start_time = time.time()\n",
    "    df = pl.read_parquet(path_parquet)\n",
    "    tiempos[\"Reading\"] = time.time() - start_time\n",
    "\n",
    "    # Filtrado\n",
    "    start_time = time.time()\n",
    "    filter_df = df.filter(pl.col(\"Arrest\") == True)\n",
    "    tiempos[\"Filtering\"] = time.time() - start_time\n",
    "\n",
    "    # Group by\n",
    "    start_time = time.time()\n",
    "    grouped_df = filter_df.group_by(\"District\")\n",
    "    tiempos[\"Grouping\"] = time.time() - start_time\n",
    "\n",
    "    # Count\n",
    "    start_time = time.time()\n",
    "    count_df = grouped_df.agg(pl.count().alias(\"count\"))\n",
    "    tiempos[\"Counting\"] = time.time() - start_time\n",
    "\n",
    "    # Sort\n",
    "    start_time = time.time()\n",
    "    sort_df = count_df.sort(\"count\", descending=True)\n",
    "    tiempos[\"Sorting\"] = time.time() - start_time\n",
    "\n",
    "    return tiempos\n",
    "\n",
    "\n",
    "def show_comparison_chart(results_df):\n",
    "    \"\"\"\n",
    "    Genera y muestra el gráfico de barras apiladas para comparar los tiempos de cada método.\n",
    "    \"\"\"\n",
    "    colors = [\"red\", \"steelblue\", \"green\", \"purple\", \"orange\", \"brown\", \"magenta\", \"cyan\"]\n",
    "    operations = [\"Reading\", \"Filtering\", \"Grouping\", \"Counting\", \"Sorting\"]\n",
    "\n",
    "    # Crear vector de ceros para la parte \"bottom\" del gráfico apilado\n",
    "    bottom = np.zeros(len(results_df[\"Method\"]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    # Graficar cada operación como una capa apilada\n",
    "    for i, operation in enumerate(operations):\n",
    "        ax.bar(results_df[\"Method\"], results_df[operation], \n",
    "               bottom=bottom, label=operation, color=colors[i])\n",
    "        bottom += results_df[operation]\n",
    "\n",
    "    ax.set_title(\"Performance Comparison\")\n",
    "    ax.set_ylabel(\"Total Time (s)\")\n",
    "    ax.legend(title=\"Operations\", loc=\"upper right\")\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "############################\n",
    "#          MAIN            #\n",
    "############################\n",
    "\n",
    "def main():\n",
    "    # Ajusta estas rutas a tus necesidades\n",
    "    path_file_csv = \"./data/Crimes_-_2001_to_Present.csv\"   # CSV original\n",
    "    path_parquet_file = \"./data_parquet\"                       # Se genera a partir del CSV\n",
    "\n",
    "    # 1) Transformar CSV a Parquet (descomentar si no existe el archivo parquet)\n",
    "    transform_csv_to_parquet(path_file_csv, path_parquet_file)\n",
    "\n",
    "    # 2) Medición de tiempos para cada método y cada formato\n",
    "    rdd_csv_times = measure_spark_rdd_csv(path_file_csv)\n",
    "    sparkdf_csv_times = measure_spark_df_csv(path_file_csv)\n",
    "    pandas_csv_times = measure_pandas_csv(path_file_csv)\n",
    "    polars_csv_times = measure_polars_csv(path_file_csv)\n",
    "\n",
    "    rdd_parquet_times = measure_spark_rdd_parquet(path_parquet_file)\n",
    "    sparkdf_parquet_times = measure_spark_df_parquet(path_parquet_file)\n",
    "    pandas_parquet_times = measure_pandas_parquet(path_parquet_file)\n",
    "    polars_parquet_times = measure_polars_parquet(path_parquet_file)\n",
    "\n",
    "    # 3) Construir un DataFrame de Pandas con los resultados\n",
    "    data = {\n",
    "        \"Method\": [\n",
    "            \"RDD CSV\",\n",
    "            \"SparkDF CSV\",\n",
    "            \"Pandas CSV\",\n",
    "            \"Polars CSV\",\n",
    "            \"RDD Parquet\",\n",
    "            \"SparkDF Parquet\",\n",
    "            \"Pandas Parquet\",\n",
    "            \"Polars Parquet\"\n",
    "        ],\n",
    "        \"Reading\": [\n",
    "            rdd_csv_times[\"Reading\"],\n",
    "            sparkdf_csv_times[\"Reading\"],\n",
    "            pandas_csv_times[\"Reading\"],\n",
    "            polars_csv_times[\"Reading\"],\n",
    "            rdd_parquet_times[\"Reading\"],\n",
    "            sparkdf_parquet_times[\"Reading\"],\n",
    "            pandas_parquet_times[\"Reading\"],\n",
    "            polars_parquet_times[\"Reading\"]\n",
    "        ],\n",
    "        \"Filtering\": [\n",
    "            rdd_csv_times[\"Filtering\"],\n",
    "            sparkdf_csv_times[\"Filtering\"],\n",
    "            pandas_csv_times[\"Filtering\"],\n",
    "            polars_csv_times[\"Filtering\"],\n",
    "            rdd_parquet_times[\"Filtering\"],\n",
    "            sparkdf_parquet_times[\"Filtering\"],\n",
    "            pandas_parquet_times[\"Filtering\"],\n",
    "            polars_parquet_times[\"Filtering\"]\n",
    "        ],\n",
    "        \"Grouping\": [\n",
    "            rdd_csv_times[\"Grouping\"],\n",
    "            sparkdf_csv_times[\"Grouping\"],\n",
    "            pandas_csv_times[\"Grouping\"],\n",
    "            polars_csv_times[\"Grouping\"],\n",
    "            rdd_parquet_times[\"Grouping\"],\n",
    "            sparkdf_parquet_times[\"Grouping\"],\n",
    "            pandas_parquet_times[\"Grouping\"],\n",
    "            polars_parquet_times[\"Grouping\"]\n",
    "        ],\n",
    "        \"Counting\": [\n",
    "            rdd_csv_times[\"Counting\"],\n",
    "            sparkdf_csv_times[\"Counting\"],\n",
    "            pandas_csv_times[\"Counting\"],\n",
    "            polars_csv_times[\"Counting\"],\n",
    "            rdd_parquet_times[\"Counting\"],\n",
    "            sparkdf_parquet_times[\"Counting\"],\n",
    "            pandas_parquet_times[\"Counting\"],\n",
    "            polars_parquet_times[\"Counting\"]\n",
    "        ],\n",
    "        \"Sorting\": [\n",
    "            rdd_csv_times[\"Sorting\"],\n",
    "            sparkdf_csv_times[\"Sorting\"],\n",
    "            pandas_csv_times[\"Sorting\"],\n",
    "            polars_csv_times[\"Sorting\"],\n",
    "            rdd_parquet_times[\"Sorting\"],\n",
    "            sparkdf_parquet_times[\"Sorting\"],\n",
    "            pandas_parquet_times[\"Sorting\"],\n",
    "            polars_parquet_times[\"Sorting\"]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    results_df = pd.DataFrame(data)\n",
    "\n",
    "    print(\"\\n====================== RESULTADOS ======================\")\n",
    "    print(results_df)\n",
    "    print(\"========================================================\\n\")\n",
    "\n",
    "    # 4) Mostrar el gráfico comparativo\n",
    "    show_comparison_chart(results_df)\n",
    "\n",
    "\n",
    "# Punto de entrada del script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab52848-6d95-40a7-af3e-e913d404ad3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spark342]",
   "language": "python",
   "name": "conda-env-spark342-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
