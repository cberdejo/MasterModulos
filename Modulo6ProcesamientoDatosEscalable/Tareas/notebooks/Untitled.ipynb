{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74b81270-c156-4967-9b6f-7d6bce8fa371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cberd\\AppData\\Local\\Temp\\ipykernel_14600\\4265885909.py:207: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  count_df = grouped_df.agg(pl.count().alias(\"count\"))\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 29.0 failed 1 times, most recent failure: Lost task 5.0 in stage 29.0 (TID 598) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:128)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2790)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2726)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2725)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2725)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1211)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2989)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2928)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2917)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:976)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2258)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2298)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1022)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:408)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1021)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:128)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 20 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 522\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;66;03m# Punto de entrada del script\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 522\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 441\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    438\u001b[0m pandas_csv_times \u001b[38;5;241m=\u001b[39m measure_pandas_csv(path_file_csv)\n\u001b[0;32m    439\u001b[0m polars_csv_times \u001b[38;5;241m=\u001b[39m measure_polars_csv(path_file_csv)\n\u001b[1;32m--> 441\u001b[0m rdd_parquet_times \u001b[38;5;241m=\u001b[39m \u001b[43mmeasure_spark_rdd_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_parquet_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    442\u001b[0m sparkdf_parquet_times \u001b[38;5;241m=\u001b[39m measure_spark_df_parquet(path_parquet_file)\n\u001b[0;32m    443\u001b[0m pandas_parquet_times \u001b[38;5;241m=\u001b[39m measure_pandas_parquet(path_parquet_file)\n",
      "Cell \u001b[1;32mIn[10], line 322\u001b[0m, in \u001b[0;36mmeasure_spark_rdd_parquet\u001b[1;34m(path_parquet)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;66;03m# Sort\u001b[39;00m\n\u001b[0;32m    321\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 322\u001b[0m rdd_sorted \u001b[38;5;241m=\u001b[39m \u001b[43mrdd_counted\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msortBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m tiempos[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSorting\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tiempos\n",
      "File \u001b[1;32mc:\\Users\\cberd\\anaconda3\\envs\\spark342\\lib\\site-packages\\pyspark\\rdd.py:1558\u001b[0m, in \u001b[0;36mRDD.sortBy\u001b[1;34m(self, keyfunc, ascending, numPartitions)\u001b[0m\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msortBy\u001b[39m(\n\u001b[0;32m   1520\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1521\u001b[0m     keyfunc: Callable[[T], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1522\u001b[0m     ascending: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1523\u001b[0m     numPartitions: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1524\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;124;03m    Sorts this RDD by the given keyfunc\u001b[39;00m\n\u001b[0;32m   1527\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;124;03m    [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1558\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeyBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[type-var]\u001b[39;49;00m\n\u001b[0;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msortByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mascending\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumPartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m   1561\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\cberd\\anaconda3\\envs\\spark342\\lib\\site-packages\\pyspark\\rdd.py:1495\u001b[0m, in \u001b[0;36mRDD.sortByKey\u001b[1;34m(self, ascending, numPartitions, keyfunc)\u001b[0m\n\u001b[0;32m   1490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapPartitions(sortPartition, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;66;03m# first compute the boundary of each part via sampling: we want to partition\u001b[39;00m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;66;03m# the key-space into bins such that the bins have roughly the same\u001b[39;00m\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;66;03m# number of (key, value) pairs falling into them\u001b[39;00m\n\u001b[1;32m-> 1495\u001b[0m rddSize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rddSize:\n\u001b[0;32m   1497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# empty RDD\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cberd\\anaconda3\\envs\\spark342\\lib\\site-packages\\pyspark\\rdd.py:2297\u001b[0m, in \u001b[0;36mRDD.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   2277\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;124;03m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[0;32m   2279\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[0;32m   2296\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cberd\\anaconda3\\envs\\spark342\\lib\\site-packages\\pyspark\\rdd.py:2272\u001b[0m, in \u001b[0;36mRDD.sum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msum\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[NumberOrArray]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumberOrArray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2252\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2253\u001b[0m \u001b[38;5;124;03m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[0;32m   2254\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[38;5;124;03m    6.0\u001b[39;00m\n\u001b[0;32m   2271\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[0;32m   2273\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\n\u001b[0;32m   2274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cberd\\anaconda3\\envs\\spark342\\lib\\site-packages\\pyspark\\rdd.py:2025\u001b[0m, in \u001b[0;36mRDD.fold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m   2020\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m acc\n\u001b[0;32m   2022\u001b[0m \u001b[38;5;66;03m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[0;32m   2023\u001b[0m \u001b[38;5;66;03m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[0;32m   2024\u001b[0m \u001b[38;5;66;03m# to the final reduce call\u001b[39;00m\n\u001b[1;32m-> 2025\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[1;32mc:\\Users\\cberd\\anaconda3\\envs\\spark342\\lib\\site-packages\\pyspark\\rdd.py:1814\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1812\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1813\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1814\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mC:\\spark-3.4.4-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\cberd\\anaconda3\\envs\\spark342\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark-3.4.4-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 29.0 failed 1 times, most recent failure: Lost task 5.0 in stage 29.0 (TID 598) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:128)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2790)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2726)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2725)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2725)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1211)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2989)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2928)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2917)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:976)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2258)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2298)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1022)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:408)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1021)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:128)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 20 more\r\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script de comparación de performance entre distintas librerías/sistemas:\n",
    "\n",
    "1. PySpark Dataframes (CSV y Parquet)\n",
    "2. PySpark RDD (CSV y Parquet)\n",
    "3. Pandas (CSV y Parquet)\n",
    "4. Polars (CSV y Parquet)\n",
    "\n",
    "Autoría original:\n",
    "- Pablo Nieto Rodríguez\n",
    "- Pablo Fontádez\n",
    "- Christian Berdejo Sánchez\n",
    "\n",
    "Adaptado para un único script y organizado en funciones.\n",
    "\n",
    "Se miden los tiempos de:\n",
    "1. Lectura\n",
    "2. Filtrado (Arrest = true)\n",
    "3. Agrupación (Group by District)\n",
    "4. Conteo\n",
    "5. Ordenamiento\n",
    "\n",
    "Al final se muestra un gráfico comparativo con los tiempos medidos.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "\n",
    "############################\n",
    "#         FUNCIONES        #\n",
    "############################\n",
    "\n",
    "def measure_spark_df_csv(path_file):\n",
    "    \"\"\"\n",
    "    Mide los tiempos de lectura, filtrado, group by, count y sort usando Spark DataFrame con un archivo CSV.\n",
    "    Devuelve un diccionario con cada operación y su tiempo.\n",
    "    \"\"\"\n",
    "    spark_session = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"SparkDataFrameCSV\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark_session.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    tiempos = {}\n",
    "\n",
    "    # Lectura\n",
    "    start_time = time.time()\n",
    "    data_frame = (spark_session.read\n",
    "                               .options(header='true', inferschema='true')\n",
    "                               .option(\"delimiter\", \",\")\n",
    "                               .option(\"timestampFormat\", \"yyyy-MM-dd\")\n",
    "                               .csv(path_file)\n",
    "                               .persist())\n",
    "    tiempos[\"Reading\"] = time.time() - start_time\n",
    "\n",
    "    # Filtrado\n",
    "    start_time = time.time()\n",
    "    filtered_df = data_frame.filter(\"Arrest = true\")\n",
    "    tiempos[\"Filtering\"] = time.time() - start_time\n",
    "\n",
    "    # Group by\n",
    "    start_time = time.time()\n",
    "    grouped_df = filtered_df.groupby(\"District\")\n",
    "    tiempos[\"Grouping\"] = time.time() - start_time\n",
    "\n",
    "    # Count\n",
    "    start_time = time.time()\n",
    "    counted_df = grouped_df.count()\n",
    "    tiempos[\"Counting\"] = time.time() - start_time\n",
    "\n",
    "    # Sort\n",
    "    start_time = time.time()\n",
    "    sorted_df = counted_df.orderBy(\"count\", ascending=False)\n",
    "    tiempos[\"Sorting\"] = time.time() - start_time\n",
    "\n",
    "    return tiempos\n",
    "\n",
    "\n",
    "def measure_spark_rdd_csv(path_file):\n",
    "    \"\"\"\n",
    "    Mide los tiempos de lectura, filtrado, group by, count y sort usando Spark RDD con un archivo CSV.\n",
    "    Devuelve un diccionario con cada operación y su tiempo.\n",
    "    \"\"\"\n",
    "    spark_session = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"SparkRDDCSV\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark_context = spark_session.sparkContext\n",
    "    spark_context.setLogLevel(\"ERROR\")\n",
    "\n",
    "    tiempos = {}\n",
    "\n",
    "    # Lectura\n",
    "    start_time = time.time()\n",
    "    rdd = spark_context.textFile(path_file)\n",
    "    tiempos[\"Reading\"] = time.time() - start_time\n",
    "\n",
    "    header = rdd.first()\n",
    "    columns = header.split(\",\")\n",
    "\n",
    "    try:\n",
    "        arrest_idx = columns.index(\"Arrest\")\n",
    "        district_idx = columns.index(\"District\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Verifica que las columnas 'Arrest' y 'District' existan en el CSV.\")\n",
    "\n",
    "    rdd_no_header = rdd.filter(lambda line: line != header)\n",
    "    rdd_parsed = rdd_no_header.map(lambda line: line.split(\",\"))\n",
    "\n",
    "    # Filtrado\n",
    "    start_time = time.time()\n",
    "    rdd_filtered = rdd_parsed.filter(\n",
    "        lambda row: len(row) > max(arrest_idx, district_idx) and row[arrest_idx].lower() == \"true\"\n",
    "    )\n",
    "    tiempos[\"Filtering\"] = time.time() - start_time\n",
    "\n",
    "    # Group by (aquí en realidad se mapea a (district,1))\n",
    "    start_time = time.time()\n",
    "    rdd_mapped = rdd_filtered.map(lambda row: (row[district_idx], 1))\n",
    "    tiempos[\"Grouping\"] = time.time() - start_time\n",
    "\n",
    "    # Count (reduceByKey)\n",
    "    start_time = time.time()\n",
    "    rdd_counted = rdd_mapped.reduceByKey(lambda a, b: a + b)\n",
    "    tiempos[\"Counting\"] = time.time() - start_time\n",
    "\n",
    "    # Sort\n",
    "    start_time = time.time()\n",
    "    rdd_sorted = rdd_counted.sortBy(lambda x: x[1], ascending=False)\n",
    "    tiempos[\"Sorting\"] = time.time() - start_time\n",
    "\n",
    "    return tiempos\n",
    "\n",
    "\n",
    "def measure_pandas_csv(path_file):\n",
    "    \"\"\"\n",
    "    Mide los tiempos de lectura, filtrado, group by, count y sort usando Pandas con un archivo CSV.\n",
    "    Devuelve un diccionario con cada operación y su tiempo.\n",
    "    \"\"\"\n",
    "    tiempos = {}\n",
    "\n",
    "    # Lectura\n",
    "    start_time = time.time()\n",
    "    df = pd.read_csv(path_file, delimiter=\",\", low_memory=False)\n",
    "    tiempos[\"Reading\"] = time.time() - start_time\n",
    "\n",
    "    # Filtrado\n",
    "    start_time = time.time()\n",
    "    filtered_df = df[df[\"Arrest\"] == True]\n",
    "    tiempos[\"Filtering\"] = time.time() - start_time\n",
    "\n",
    "    # Agrupación\n",
    "    start_time = time.time()\n",
    "    grouped_df = filtered_df.groupby(\"District\").size()\n",
    "    tiempos[\"Grouping\"] = time.time() - start_time\n",
    "\n",
    "    # Conteo (en este caso, el size ya devolvió el conteo, pero separamos la medición)\n",
    "    start_time = time.time()\n",
    "    counted_df = grouped_df.reset_index(name=\"count\")\n",
    "    tiempos[\"Counting\"] = time.time() - start_time\n",
    "\n",
    "    # Ordenamiento\n",
    "    start_time = time.time()\n",
    "    sorted_df = counted_df.sort_values(by=\"count\", ascending=False)\n",
    "    tiempos[\"Sorting\"] = time.time() - start_time\n",
    "\n",
    "    return tiempos\n",
    "\n",
    "\n",
    "def measure_polars_csv(path_file):\n",
    "    \"\"\"\n",
    "    Mide los tiempos de lectura, filtrado, group by, count y sort usando Polars con un archivo CSV.\n",
    "    Devuelve un diccionario con cada operación y su tiempo.\n",
    "    \"\"\"\n",
    "    tiempos = {}\n",
    "\n",
    "    # Lectura\n",
    "    start_time = time.time()\n",
    "    df = pl.read_csv(path_file)\n",
    "    tiempos[\"Reading\"] = time.time() - start_time\n",
    "\n",
    "    # Filtrado\n",
    "    start_time = time.time()\n",
    "    filter_df = df.filter(pl.col(\"Arrest\") == True)\n",
    "    tiempos[\"Filtering\"] = time.time() - start_time\n",
    "\n",
    "    # Agrupación (group_by)\n",
    "    start_time = time.time()\n",
    "    grouped_df = filter_df.group_by(\"District\")\n",
    "    tiempos[\"Grouping\"] = time.time() - start_time\n",
    "\n",
    "    # Conteo\n",
    "    start_time = time.time()\n",
    "    count_df = grouped_df.agg(pl.count().alias(\"count\"))\n",
    "    tiempos[\"Counting\"] = time.time() - start_time\n",
    "\n",
    "    # Ordenamiento\n",
    "    start_time = time.time()\n",
    "    sort_df = count_df.sort(\"count\", descending=True)\n",
    "    tiempos[\"Sorting\"] = time.time() - start_time\n",
    "\n",
    "    return tiempos\n",
    "\n",
    "\n",
    "def transform_csv_to_parquet(path_file_csv, path_file_parquet=\"data_parquet\"):\n",
    "    \"\"\"\n",
    "    Lee un CSV usando Pandas y lo transforma a parquet (usando pyarrow) en la ruta especificada.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path_file_csv)\n",
    "    df.to_parquet(path_file_parquet, engine='pyarrow')\n",
    "\n",
    "\n",
    "def measure_spark_df_parquet(path_parquet):\n",
    "    \"\"\"\n",
    "    Mide los tiempos de lectura, filtrado, group by, count y sort usando Spark DataFrame con un archivo Parquet.\n",
    "    Devuelve un diccionario con cada operación y su tiempo.\n",
    "    \"\"\"\n",
    "    spark_session = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"SparkDataFrameParquet\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark_session.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    tiempos = {}\n",
    "\n",
    "    # Lectura\n",
    "    start_time = time.time()\n",
    "    data_frame = (spark_session.read\n",
    "                               .options(header='true', inferschema='true')\n",
    "                               .option(\"delimiter\", \",\")\n",
    "                               .option(\"timestampFormat\", \"yyyy-MM-dd\")\n",
    "                               .parquet(path_parquet)\n",
    "                               .persist())\n",
    "    tiempos[\"Reading\"] = time.time() - start_time\n",
    "\n",
    "    # Filtrado\n",
    "    start_time = time.time()\n",
    "    filtered_df = data_frame.filter(\"Arrest = true\")\n",
    "    tiempos[\"Filtering\"] = time.time() - start_time\n",
    "\n",
    "    # Group by\n",
    "    start_time = time.time()\n",
    "    grouped_df = filtered_df.groupby(\"District\")\n",
    "    tiempos[\"Grouping\"] = time.time() - start_time\n",
    "\n",
    "    # Count\n",
    "    start_time = time.time()\n",
    "    counted_df = grouped_df.count()\n",
    "    tiempos[\"Counting\"] = time.time() - start_time\n",
    "\n",
    "    # Sort\n",
    "    start_time = time.time()\n",
    "    sorted_df = counted_df.orderBy(\"count\", ascending=False)\n",
    "    tiempos[\"Sorting\"] = time.time() - start_time\n",
    "\n",
    "    return tiempos\n",
    "\n",
    "\n",
    "def measure_spark_rdd_parquet(path_parquet):\n",
    "    \"\"\"\n",
    "    Mide los tiempos de lectura, filtrado, group by, count y sort usando Spark RDD\n",
    "    leyendo primero un DataFrame parquet y convirtiéndolo a RDD.\n",
    "    Devuelve un diccionario con cada operación y su tiempo.\n",
    "    \"\"\"\n",
    "\n",
    "    conf = SparkConf() \\\n",
    "    .set(\"spark.driver.memory\", \"6g\") \\\n",
    "    .set(\"spark.executor.memory\", \"6g\") \\\n",
    "    .set(\"spark.network.timeout\", \"600s\") \\\n",
    "    .set(\"spark.executor.heartbeatInterval\", \"120s\")\n",
    "    \n",
    "    spark_session = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(conf=conf)\\\n",
    "        .appName(\"SparkRDDParquet\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark_context = spark_session.sparkContext\n",
    "    spark_context.setLogLevel(\"ERROR\")\n",
    "\n",
    "    tiempos = {}\n",
    "\n",
    "    # Lectura (Spark no lee parquet con RDD directamente, se lee DF y se convierte a RDD)\n",
    "    start_time = time.time()\n",
    "    spark_df = spark_session.read.parquet(path_parquet)\n",
    "    rdd = spark_df.rdd\n",
    "    tiempos[\"Reading\"] = time.time() - start_time\n",
    "\n",
    "    # Filtrado\n",
    "    start_time = time.time()\n",
    "    rdd_filtered = rdd.filter(lambda row: row.Arrest is not None and row.Arrest == True)\n",
    "    tiempos[\"Filtering\"] = time.time() - start_time\n",
    "\n",
    "    \n",
    "    # Group by (map -> reduceByKey)\n",
    "    start_time = time.time()\n",
    "    rdd_mapped = rdd_filtered.map(lambda row: (row.District, 1) \n",
    "    if row.District is not None else (\"Unknown\", 1))\n",
    "    tiempos[\"Grouping\"] = time.time() - start_time\n",
    "\n",
    "    # Count\n",
    "    start_time = time.time()\n",
    "    rdd_counted = rdd_mapped.reduceByKey(lambda a, b: a + b)\n",
    "    tiempos[\"Counting\"] = time.time() - start_time\n",
    "\n",
    "    # Sort\n",
    "    start_time = time.time()\n",
    "    rdd_sorted = rdd_counted.sortBy(lambda x: x[1], ascending=False)\n",
    "    tiempos[\"Sorting\"] = time.time() - start_time\n",
    "\n",
    "    return tiempos\n",
    "\n",
    "\n",
    "def measure_pandas_parquet(path_parquet):\n",
    "    \"\"\"\n",
    "    Mide los tiempos de lectura, filtrado, group by, count y sort usando Pandas con un archivo Parquet.\n",
    "    Devuelve un diccionario con cada operación y su tiempo.\n",
    "    \"\"\"\n",
    "    tiempos = {}\n",
    "\n",
    "    # Lectura\n",
    "    start_time = time.time()\n",
    "    df = pd.read_parquet(path_parquet, engine=\"pyarrow\")\n",
    "    tiempos[\"Reading\"] = time.time() - start_time\n",
    "\n",
    "    # Filtrado\n",
    "    start_time = time.time()\n",
    "    df_filtrado = df[df[\"Arrest\"] == True]\n",
    "    tiempos[\"Filtering\"] = time.time() - start_time\n",
    "\n",
    "    # Agrupación\n",
    "    start_time = time.time()\n",
    "    df_agrupado = df_filtrado.groupby(\"District\").size().reset_index(name=\"count\")\n",
    "    tiempos[\"Grouping\"] = time.time() - start_time\n",
    "\n",
    "    # Conteo (aquí está incluido en size() y reset_index)\n",
    "    start_time = time.time()\n",
    "    df_count = df_filtrado.groupby(\"District\")[\"Arrest\"].count()\n",
    "    tiempos[\"Counting\"] = time.time() - start_time\n",
    "\n",
    "    # Ordenamiento\n",
    "    start_time = time.time()\n",
    "    df_sorted = df_agrupado.sort_values(by=\"count\", ascending=False)\n",
    "    tiempos[\"Sorting\"] = time.time() - start_time\n",
    "\n",
    "    return tiempos\n",
    "\n",
    "\n",
    "def measure_polars_parquet(path_parquet):\n",
    "    \"\"\"\n",
    "    Mide los tiempos de lectura, filtrado, group by, count y sort usando Polars con un archivo Parquet.\n",
    "    Devuelve un diccionario con cada operación y su tiempo.\n",
    "    \"\"\"\n",
    "    tiempos = {}\n",
    "\n",
    "    # Lectura\n",
    "    start_time = time.time()\n",
    "    df = pl.read_parquet(path_parquet)\n",
    "    tiempos[\"Reading\"] = time.time() - start_time\n",
    "\n",
    "    # Filtrado\n",
    "    start_time = time.time()\n",
    "    filter_df = df.filter(pl.col(\"Arrest\") == True)\n",
    "    tiempos[\"Filtering\"] = time.time() - start_time\n",
    "\n",
    "    # Group by\n",
    "    start_time = time.time()\n",
    "    grouped_df = filter_df.group_by(\"District\")\n",
    "    tiempos[\"Grouping\"] = time.time() - start_time\n",
    "\n",
    "    # Count\n",
    "    start_time = time.time()\n",
    "    count_df = grouped_df.agg(pl.count().alias(\"count\"))\n",
    "    tiempos[\"Counting\"] = time.time() - start_time\n",
    "\n",
    "    # Sort\n",
    "    start_time = time.time()\n",
    "    sort_df = count_df.sort(\"count\", descending=True)\n",
    "    tiempos[\"Sorting\"] = time.time() - start_time\n",
    "\n",
    "    return tiempos\n",
    "\n",
    "\n",
    "def show_comparison_chart(results_df):\n",
    "    \"\"\"\n",
    "    Genera y muestra el gráfico de barras apiladas para comparar los tiempos de cada método.\n",
    "    \"\"\"\n",
    "    colors = [\"red\", \"steelblue\", \"green\", \"purple\", \"orange\", \"brown\", \"magenta\", \"cyan\"]\n",
    "    operations = [\"Reading\", \"Filtering\", \"Grouping\", \"Counting\", \"Sorting\"]\n",
    "\n",
    "    # Crear vector de ceros para la parte \"bottom\" del gráfico apilado\n",
    "    bottom = np.zeros(len(results_df[\"Method\"]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    # Graficar cada operación como una capa apilada\n",
    "    for i, operation in enumerate(operations):\n",
    "        ax.bar(results_df[\"Method\"], results_df[operation], \n",
    "               bottom=bottom, label=operation, color=colors[i])\n",
    "        bottom += results_df[operation]\n",
    "\n",
    "    ax.set_title(\"Performance Comparison\")\n",
    "    ax.set_ylabel(\"Total Time (s)\")\n",
    "    ax.legend(title=\"Operations\", loc=\"upper right\")\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "############################\n",
    "#          MAIN            #\n",
    "############################\n",
    "\n",
    "def main():\n",
    "    # Ajusta estas rutas a tus necesidades\n",
    "    path_file_csv = \"./data/Crimes_-_2001_to_Present.csv\"   # CSV original\n",
    "    path_parquet_file = \"./data_parquet\"                       # Se genera a partir del CSV\n",
    "\n",
    "    # 1) Transformar CSV a Parquet (descomentar si no existe el archivo parquet)\n",
    "    transform_csv_to_parquet(path_file_csv, path_parquet_file)\n",
    "\n",
    "    # 2) Medición de tiempos para cada método y cada formato\n",
    "    rdd_csv_times = measure_spark_rdd_csv(path_file_csv)\n",
    "    sparkdf_csv_times = measure_spark_df_csv(path_file_csv)\n",
    "    pandas_csv_times = measure_pandas_csv(path_file_csv)\n",
    "    polars_csv_times = measure_polars_csv(path_file_csv)\n",
    "\n",
    "    rdd_parquet_times = measure_spark_rdd_parquet(path_parquet_file)\n",
    "    sparkdf_parquet_times = measure_spark_df_parquet(path_parquet_file)\n",
    "    pandas_parquet_times = measure_pandas_parquet(path_parquet_file)\n",
    "    polars_parquet_times = measure_polars_parquet(path_parquet_file)\n",
    "\n",
    "    # 3) Construir un DataFrame de Pandas con los resultados\n",
    "    data = {\n",
    "        \"Method\": [\n",
    "            \"RDD CSV\",\n",
    "            \"SparkDF CSV\",\n",
    "            \"Pandas CSV\",\n",
    "            \"Polars CSV\",\n",
    "            \"RDD Parquet\",\n",
    "            \"SparkDF Parquet\",\n",
    "            \"Pandas Parquet\",\n",
    "            \"Polars Parquet\"\n",
    "        ],\n",
    "        \"Reading\": [\n",
    "            rdd_csv_times[\"Reading\"],\n",
    "            sparkdf_csv_times[\"Reading\"],\n",
    "            pandas_csv_times[\"Reading\"],\n",
    "            polars_csv_times[\"Reading\"],\n",
    "            rdd_parquet_times[\"Reading\"],\n",
    "            sparkdf_parquet_times[\"Reading\"],\n",
    "            pandas_parquet_times[\"Reading\"],\n",
    "            polars_parquet_times[\"Reading\"]\n",
    "        ],\n",
    "        \"Filtering\": [\n",
    "            rdd_csv_times[\"Filtering\"],\n",
    "            sparkdf_csv_times[\"Filtering\"],\n",
    "            pandas_csv_times[\"Filtering\"],\n",
    "            polars_csv_times[\"Filtering\"],\n",
    "            rdd_parquet_times[\"Filtering\"],\n",
    "            sparkdf_parquet_times[\"Filtering\"],\n",
    "            pandas_parquet_times[\"Filtering\"],\n",
    "            polars_parquet_times[\"Filtering\"]\n",
    "        ],\n",
    "        \"Grouping\": [\n",
    "            rdd_csv_times[\"Grouping\"],\n",
    "            sparkdf_csv_times[\"Grouping\"],\n",
    "            pandas_csv_times[\"Grouping\"],\n",
    "            polars_csv_times[\"Grouping\"],\n",
    "            rdd_parquet_times[\"Grouping\"],\n",
    "            sparkdf_parquet_times[\"Grouping\"],\n",
    "            pandas_parquet_times[\"Grouping\"],\n",
    "            polars_parquet_times[\"Grouping\"]\n",
    "        ],\n",
    "        \"Counting\": [\n",
    "            rdd_csv_times[\"Counting\"],\n",
    "            sparkdf_csv_times[\"Counting\"],\n",
    "            pandas_csv_times[\"Counting\"],\n",
    "            polars_csv_times[\"Counting\"],\n",
    "            rdd_parquet_times[\"Counting\"],\n",
    "            sparkdf_parquet_times[\"Counting\"],\n",
    "            pandas_parquet_times[\"Counting\"],\n",
    "            polars_parquet_times[\"Counting\"]\n",
    "        ],\n",
    "        \"Sorting\": [\n",
    "            rdd_csv_times[\"Sorting\"],\n",
    "            sparkdf_csv_times[\"Sorting\"],\n",
    "            pandas_csv_times[\"Sorting\"],\n",
    "            polars_csv_times[\"Sorting\"],\n",
    "            rdd_parquet_times[\"Sorting\"],\n",
    "            sparkdf_parquet_times[\"Sorting\"],\n",
    "            pandas_parquet_times[\"Sorting\"],\n",
    "            polars_parquet_times[\"Sorting\"]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    results_df = pd.DataFrame(data)\n",
    "\n",
    "    print(\"\\n====================== RESULTADOS ======================\")\n",
    "    print(results_df)\n",
    "    print(\"========================================================\\n\")\n",
    "\n",
    "    # 4) Mostrar el gráfico comparativo\n",
    "    show_comparison_chart(results_df)\n",
    "\n",
    "\n",
    "# Punto de entrada del script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab52848-6d95-40a7-af3e-e913d404ad3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spark342]",
   "language": "python",
   "name": "conda-env-spark342-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
