{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble comparative (Obligatory)\n",
    "## Christian Berdejo Sánchez\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "warnings.filterwarnings(action='ignore')                  # Turn off the warnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "data = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n"
     ]
    }
   ],
   "source": [
    "# Explanatory variables.\n",
    "X = data['data']\n",
    "print(data['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 13)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.str_('class_2'), np.str_('class_1'), np.str_('class_0')]\n"
     ]
    }
   ],
   "source": [
    "# Response variable.\n",
    "Y = data['target']\n",
    "label = list(data['target_names'])\n",
    "label.reverse()\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separamos en conjunto de entreno y conjunto de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=82)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learners classificators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree accuracy : 0.9259259259259259\n"
     ]
    }
   ],
   "source": [
    "# Classification Tree.\n",
    "\n",
    "DTC = DecisionTreeClassifier()\n",
    "#Búsqueda por rejilla\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rejilla = {'criterion':['gini','entropy','log_loss'], 'max_depth':[3,5,7], 'min_samples_split':[2,4,8,16]}\n",
    "\n",
    "DTCGCV = GridSearchCV(estimator=DTC,param_grid=rejilla,scoring='accuracy',cv=5)\n",
    "\n",
    "DTCGCV.fit(X_train,y_train)\n",
    "DTC_best_estimator = DTCGCV.best_estimator_\n",
    "\n",
    "y_pred = DTC_best_estimator.predict(X_test)\n",
    "print( \"Tree accuracy : \" + str(accuracy_score(y_pred,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy : 0.9259259259259259\n"
     ]
    }
   ],
   "source": [
    "# Classification with KNN\n",
    "KNN = KNeighborsClassifier()\n",
    "rejilla = {'n_neighbors':[3,5,7,9,11],'weights':['uniform','distance'],'metric':['euclidean','cosine','manhattan']}\n",
    "\n",
    "KNNGCV = GridSearchCV(estimator=KNN,param_grid=rejilla,scoring='accuracy',cv=10)\n",
    "\n",
    "KNNGCV.fit(X_train,y_train)\n",
    "KNN_best_estimator = KNNGCV.best_estimator_\n",
    "\n",
    "Y_pred = KNN_best_estimator.predict(X_test)\n",
    "print( \"KNN accuracy : \" + str(accuracy_score(y_pred,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC accuracy : 0.9259259259259259\n"
     ]
    }
   ],
   "source": [
    "# Classification with svc.\n",
    "SVC = SVC()\n",
    "rejilla = {'C':[0.1,1,10,100], 'kernel':['linear','rbf','sigmoid'],'gamma':[0.01,0.1,0.2,0.3],'degree':[2,3,4],'coef0':[0.1,0.5,1.0]}\n",
    "\n",
    "SVCGCV = GridSearchCV(estimator=SVC,param_grid=rejilla,scoring='accuracy',cv=5)\n",
    "SVCGCV.fit(X_train,y_train)\n",
    "\n",
    "SVC_best_estimator = SVCGCV.best_estimator_\n",
    "\n",
    "y_pred = SVC_best_estimator.predict(X_test)\n",
    "print( \"SVC accuracy : \" + str(accuracy_score(y_pred,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Classification with Naive Bayes\n",
    "NB = GaussianNB()\n",
    "rejilla = {'var_smoothing': np.logspace(0, -9, num=100)}\n",
    "NBGCV = GridSearchCV(estimator=NB, param_grid=rejilla, scoring='accuracy', cv=5)\n",
    "NBGCV.fit(X_train, y_train)\n",
    "\n",
    "NB_best_estimator = NBGCV.best_estimator_\n",
    "\n",
    "y_pred = NB_best_estimator.predict(X_test)\n",
    "print(\"Naive Bayes accuracy: \" + str(accuracy_score(y_pred,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble classificators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Accuracy : 0.9259259259259259\n"
     ]
    }
   ],
   "source": [
    "#HARD VOTING\n",
    "VCH = VotingClassifier(estimators=[('Tree',DTC_best_estimator),('knn',KNN_best_estimator),('SVC',SVC_best_estimator)],voting='hard')             # voting = 'hard'.\n",
    "VCH.fit(X_train, y_train)\n",
    "y_pred = VCH.predict(X_test)\n",
    "print( \"Voting Classifier Accuracy : \" + str(accuracy_score(y_pred,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Accuracy : 0.9259259259259259\n"
     ]
    }
   ],
   "source": [
    "VCS = VotingClassifier(estimators=[('Tree',DTC_best_estimator),('knn',KNN_best_estimator),('SVC',NB_best_estimator)],voting='soft')             # voting = 'hard'.\n",
    "VCS.fit(X_train, y_train)\n",
    "y_pred = VCS.predict(X_test)\n",
    "print( \"Voting Classifier Accuracy : \" + str(accuracy_score(y_pred,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Comparative \n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Model</th>\n",
    "        <th>Accuracy</th>   \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Decision Tree Classifier</td>\n",
    "        <td>0.907</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>KNeighboors Classifier</td>\n",
    "        <td>0.926</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td>SVC</td>\n",
    "        <td>0.926</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>GaussianNB</td>\n",
    "        <td>0.944</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Hard voting (DTC, KNN, SVC)</td>\n",
    "        <td>0.944</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Soft voting (DTC, KNN, GNB)</td>\n",
    "        <td>0.926</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### Explicación\n",
    "Como se puede ver los modelos que mejor se comportan son el GaussianNB y el Hard voting, ambos con un accuracy de 0.944. Pero se puede observar que para todos los modelos el accuracy es muy similar, esto se debe a que el dataset es muy sencillo y los modelos no tienen problemas para clasificarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Boosting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.9259259259259259\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Modelo base (un árbol de decisión simple)\n",
    "base_model = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Definir AdaBoost con búsqueda de hiperparámetros\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "ada = AdaBoostClassifier(base_model)\n",
    "grid_search = GridSearchCV(ada, param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_ada = grid_search.best_estimator_\n",
    "y_pred = best_ada.predict(X_test)\n",
    "\n",
    "print(\"AdaBoost Accuracy:\", str(accuracy_score(y_pred,y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al probar un algoritmo de boosting ensemble como AdaBoostClassifier, se obtiene un accuracy de 0.926, lo cual es peor que los modelos individuales. Esto se debe a que el dataset es muy sencillo y no se necesita de un algoritmo de boosting para clasificarlo.\n",
    "\n",
    "Sería interesante cambiar el dataset por uno mas complejo y ver los resultados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
